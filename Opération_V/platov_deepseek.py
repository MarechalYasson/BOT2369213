import json
import time
import requests
import base64
import os
import logging
import re
import hashlib
import random
from dotenv import load_dotenv
from requests.exceptions import RequestException, Timeout, HTTPError

# Chargement des variables d'environnement
load_dotenv()
API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-reasoner")  # Optimisation catalogue
DEEPSEEK_MODEL_TEXT = os.getenv("DEEPSEEK_MODEL_TEXT", "deepseek-chat")  # GÃ©nÃ©ration de descriptions texte
DEEPSEEK_MODEL_VISION = os.getenv("DEEPSEEK_MODEL_VISION", "deepseek-vl")  # Vision â†’ TODO

# Nouveaux modÃ¨les OpenAI
OPENAI_MODEL_CATALOG = os.getenv("OPENAI_MODEL_CATALOG", "gpt-4-turbo")
OPENAI_MODEL_TEXT = os.getenv("OPENAI_MODEL_TEXT", "gpt-3.5-turbo")
OPENAI_MODEL_VISION = os.getenv("OPENAI_MODEL_VISION", "gpt-4-vision-preview")

# Configuration du logger unique
logger = logging.getLogger('deepseek')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# API endpoints
DEEPSEEK_URL = "https://api.deepseek.com/v1/chat/completions"
OPENAI_URL = "https://api.openai.com/v1/chat/completions"
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

TEMP_FILE = "vinted_temp.json"
FINAL_FILE = "vinted_data.json"
CACHE_DIR = ".cache"
CACHE_FILE = os.path.join(CACHE_DIR, "catalog_cache.json")

MAX_RETRIES = 3
MAX_PROMPT_LENGTH = 10000

# Prompt pour optimisation catalogue
PROMPT_OPTIMISATION_CATALOGUE = """
Tu es un expert du site Vinted et de lâ€™optimisation de catalogues e-commerce.

Voici un catalogue brut contenant :
â€“ Des catÃ©gories avec leurs IDs, noms et sous-catÃ©gories (Â« children Â»)
â€“ Des marques avec ID et nom
â€“ Des filtres globaux et par catÃ©gorie

âš™ï¸ Ta mission est de nettoyer et optimiser ce catalogue sans perdre dâ€™informations essentielles. Respecte STRICTEMENT les consignes suivantes :

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ” 1. CATEGORIES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Conserve les champs Â« id Â» (entier) et Â« name Â» (chaÃ®ne de caractÃ¨res).
- Supprime uniquement les catÃ©gories sans ID ET sans nom.
- Trie les catÃ©gories et leurs enfants par ordre alphabÃ©tique sur le champ Â« name Â».
- Conserve la hiÃ©rarchie parent > enfant dans le champ Â« children Â».
- Corrige les noms (supprime les espaces doublons, normalise la casse, enlÃ¨ve les caractÃ¨res spÃ©ciaux inutiles).
- Ne modifie JAMAIS un ID existant.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ” 2. MARQUES (brands)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Chaque marque est un objet { "id": <entier ou null>, "name": <chaÃ®ne> }.
- Conserve toutes les marques, mÃªme si lâ€™ID est absent (mettre "id": null).
- Trie les marques par ordre alphabÃ©tique du champ Â« name Â».
- Supprime uniquement les doublons exacts (mÃªme Â« id Â» ET mÃªme Â« name Â»).
- Ne modifie PAS les IDs.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ” 3. FILTRES PAR CATÃ‰GORIE (Â« filters Â») et FILTRES GLOBAUX (Â« global_filters Â»)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Structure attendue : un objet JSON avec pour chaque catÃ©gorie un sous-objet par type de filtre.
- Types de filtres Ã  traiter (mÃªme structure globale et locale) :
    - size, color, status, material, season, gender,
    - type (ex : T-shirts, Jeans),
    - style (ex : Streetwear, Classique),
    - pattern (ex : Uni, RayÃ©),
    - brand_type (ex : Luxe, Premium),
    - occasion (ex : Mariage, Sport),
    - tech_features (ex : ImpermÃ©able),
    - length (ex : Court, Long)

- Chaque filtre est une liste dâ€™objets { "id": <int ou null>, "name": <string> }.
- Trie les options de chaque filtre par ordre alphabÃ©tique sur Â« name Â».
- Ne supprime PAS les options qui possÃ¨dent un ID, mÃªme si le nom est vide.
- Supprime uniquement les filtres sans ID et sans nom.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ” 4. STRUCTURE GLOBALE ATTENDUE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RÃ©ponds UNIQUEMENT avec le JSON final optimisÃ©, dans cette structure :

{
  "timestamp": <timestamp_unix>,
  "categories": [ ... ],
  "brands": [ ... ],
  "filters": {
    "<category_id>": {
      "size": [ ... ],
      "color": [ ... ],
      "status": [ ... ],
      "material": [ ... ],
      "season": [ ... ],
      "gender": [ ... ],
      "type": [ ... ],
      "style": [ ... ],
      "pattern": [ ... ],
      "brand_type": [ ... ],
      "occasion": [ ... ],
      "tech_features": [ ... ],
      "length": [ ... ]
    }
  },
  "global_filters": {
      "size": [ ... ],
      "color": [ ... ],
      "status": [ ... ],
      "material": [ ... ],
      "season": [ ... ],
      "gender": [ ... ],
      "type": [ ... ],
      "style": [ ... ],
      "pattern": [ ... ],
      "brand_type": [ ... ],
      "occasion": [ ... ],
      "tech_features": [ ... ],
      "length": [ ... ]
  }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš ï¸ RÃ¨gles finales impÃ©ratives
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€“ Ne change PAS la structure des clÃ©s JSON.
â€“ Ne rÃ©ponds PAS avec du texte explicatif, seulement le JSON final.
â€“ Tous les tableaux doivent Ãªtre triÃ©s alphabÃ©tiquement par Â« name Â».
â€“ Ne fusionne PAS les filtres globaux et par catÃ©gorie.
â€“ Ne change PAS les IDs.
"""

# ===== ARCHITECTURE UNIFIÃ‰E AMÃ‰LIORÃ‰E =====
def call_ai_api(payload, endpoint="chat", fallback_model="gpt-4-turbo", response_format="text",
                model_env_key="DEEPSEEK_MODEL"):
    """Appel unifiÃ© aux APIs d'IA avec fallback intelligent"""
    # DÃ©termination du timeout basÃ© sur le modÃ¨le
    model_timeout = 180 if "gpt-4" in fallback_model else 90

    # Tentative DeepSeek en premier
    try:
        deepseek_payload = payload.copy()

        # Configuration spÃ©cifique pour Vision
        if endpoint == "vision":
            logger.info("ğŸ”„ Vision demandÃ©e : DeepSeek non disponible, utilisation de OpenAI Vision")
            return call_openai_vision_api(payload, fallback_model, timeout=model_timeout)

        # SÃ©lection du modÃ¨le DeepSeek appropriÃ©
        model_map = {
            "DEEPSEEK_MODEL": DEEPSEEK_MODEL,
            "DEEPSEEK_MODEL_TEXT": DEEPSEEK_MODEL_TEXT,
            "DEEPSEEK_MODEL_VISION": DEEPSEEK_MODEL_VISION
        }
        deepseek_model = model_map.get(model_env_key, DEEPSEEK_MODEL)

        if "model" not in deepseek_payload:
            deepseek_payload["model"] = deepseek_model

        # Format JSON si nÃ©cessaire
        if response_format == "json_object":
            deepseek_payload["response_format"] = {"type": "json_object"}

        logger.info(f"Tentative DeepSeek ({deepseek_model})...")
        start_time = time.time()
        response = requests.post(
            DEEPSEEK_URL,
            json=deepseek_payload,
            headers=HEADERS,
            timeout=model_timeout
        )
        response.raise_for_status()

        result = response.json()
        if "choices" not in result or not result["choices"]:
            raise ValueError("RÃ©ponse DeepSeek invalide: clÃ© 'choices' manquante")

        content = result["choices"][0]["message"]["content"].strip()
        duration = time.time() - start_time
        logger.info(f"âœ… DeepSeek rÃ©ussi en {duration:.2f}s")
        return content

    except Exception as e:
        logger.warning(f"DeepSeek Ã©chouÃ© â†’ fallback {fallback_model}: {str(e)}")

        # Fallback OpenAI
        try:
            if not OPENAI_API_KEY:
                raise RuntimeError("ClÃ© OpenAI manquante")

            headers = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }

            # SÃ©lection automatique du modÃ¨le si non spÃ©cifiÃ©
            if fallback_model == "auto":
                fallback_model = OPENAI_MODEL_CATALOG if response_format == "json_object" else OPENAI_MODEL_TEXT

            openai_payload = {
                "model": fallback_model,
                "messages": payload["messages"],
                "temperature": payload.get("temperature", 0.7),
                "max_tokens": payload.get("max_tokens", 300)
            }

            # Correction cruciale: format JSON pour OpenAI
            if response_format == "json_object":
                # Format requis par OpenAI (string)
                openai_payload["response_format"] = "json_object"

            # Gestion spÃ©cifique pour Vision
            if endpoint == "vision":
                return call_openai_vision_api(payload, fallback_model, timeout=model_timeout)

            logger.info(f"Tentative OpenAI ({fallback_model})...")
            start_time = time.time()
            response = requests.post(
                OPENAI_URL,
                json=openai_payload,
                headers=headers,
                timeout=model_timeout
            )
            response.raise_for_status()

            result = response.json()
            if "choices" not in result or not result["choices"]:
                raise ValueError("RÃ©ponse OpenAI invalide: pas de choix")

            content = result["choices"][0]["message"]["content"].strip()
            duration = time.time() - start_time
            logger.info(f"âœ… OpenAI rÃ©ussi en {duration:.2f}s")
            return content

        except Exception as fallback_error:
            logger.error(f"Ã‰chec complet API: {str(fallback_error)}")
            raise RuntimeError(f"Ã‰chec DeepSeek + OpenAI: {str(fallback_error)}")


def call_openai_vision_api(payload, fallback_model, timeout=90):
    """Appelle OpenAI Vision (GPT-4 Vision) pour traiter une image en base64."""
    try:
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }

        # Payload complet avec fallback_model (gpt-4-vision-preview normalement)
        openai_payload = {
            "model": fallback_model,
            "messages": payload.get("messages", []),
            "max_tokens": payload.get("max_tokens", 250),
            "temperature": payload.get("temperature", 0.7)
        }

        url = "https://api.openai.com/v1/chat/completions"
        logger.info(f"ğŸ”„ Tentative OpenAI Vision ({fallback_model})...")

        response = requests.post(url, headers=headers, json=openai_payload, timeout=timeout)
        response.raise_for_status()

        data = response.json()
        return data["choices"][0]["message"]["content"]

    except Exception as e:
        logger.error(f"ğŸ”¥ Erreur OpenAI Vision: {str(e)}", exc_info=True)
        raise



# ===== FONCTIONS EXISTANTES MODIFIÃ‰ES =====
def optimize_catalog_via_api(prompt):
    """Optimisation du catalogue via API avec fallback contrÃ´lÃ©"""
    try:
        payload = {
            "messages": [
                {"role": "system", "content": "Tu es un assistant expert Vinted."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1,
            "max_tokens": 4096
        }

        optimized_json = call_ai_api(
            payload,
            endpoint="chat",
            fallback_model=OPENAI_MODEL_CATALOG,
            response_format="json_object",
            model_env_key="DEEPSEEK_MODEL"  # Utilise le modÃ¨le catalogue
        )

        # Extraction et validation
        parsed_data = extract_json_from_text(optimized_json)
        validate_json_structure(parsed_data)
        return parsed_data

    except Exception as e:
        logger.error(f"Ã‰chec traitement catalogue: {str(e)}")
        raise


def generer_description_depuis_image(image_path):
    """GÃ©nÃ©ration de description Ã  partir d'une image avec fallback"""
    try:
        with open(image_path, "rb") as image_file:
            encoded_image = base64.b64encode(image_file.read()).decode('utf-8')

        payload = {
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "text", "text": PROMPT_COMMUN},
                    {"type": "image_url", "image_url": f"data:image/jpeg;base64,{encoded_image}"}
                ]
            }],
            "temperature": 0.7,
            "max_tokens": 250
        }

        # Utilisation du fallback OpenAI Vision
        return call_ai_api(
            payload,
            endpoint="vision",
            fallback_model=OPENAI_MODEL_VISION
        )

    except Exception as e:
        logger.error(f"Erreur gÃ©nÃ©ration image: {str(e)}")
        return f"âŒ Erreur lors de la gÃ©nÃ©ration de la description"


def generer_description_depuis_texte(nom_produit):
    """GÃ©nÃ©ration de description Ã  partir d'un texte avec fallback"""
    try:
        prompt = f"{PROMPT_COMMUN}\n\nProduit Ã  dÃ©crire : {nom_produit}"

        payload = {
            "messages": [
                {"role": "system", "content": "Tu es un assistant expert Vinted."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 250
        }

        return call_ai_api(
            payload,
            endpoint="chat",
            fallback_model=OPENAI_MODEL_TEXT,
            model_env_key="DEEPSEEK_MODEL_TEXT"  # Utilise le modÃ¨le texte
        )

    except Exception as e:
        logger.error(f"Erreur gÃ©nÃ©ration texte: {str(e)}")
        return f"âŒ Erreur lors de la gÃ©nÃ©ration de la description"


# ===== FONCTIONS EXISTANTES CORRIGÃ‰ES =====
def process_catalog_chunks(raw_data):
    """Traite le catalogue en chunks si nÃ©cessaire avec dÃ©coupage intelligent"""
    full_prompt = build_catalog_prompt(raw_data)

    if len(full_prompt) <= MAX_PROMPT_LENGTH:
        logger.info("Prompt complet dans les limites. Traitement en une seule requÃªte.")
        return optimize_catalog_via_api(full_prompt)

    logger.warning(f"Prompt trop long ({len(full_prompt)} > {MAX_PROMPT_LENGTH}). DÃ©coupage intelligent...")

    categories = raw_data.get('categories', [])
    brands = raw_data.get('brands', [])
    global_filters = raw_data.get('global_filters', {})

    merged_data = {
        "timestamp": int(time.time()),
        "categories": [],
        "brands": [],
        "filters": {},
        "global_filters": global_filters
    }

    current_chunk = []
    current_size = 0
    chunks = []

    for category in categories:
        cat_size = len(category.get('name', '')) + sum(
            len(child.get('name', '')) for child in category.get('children', [])[:5])

        if current_size + cat_size > MAX_PROMPT_LENGTH * 0.8 and current_chunk:
            chunks.append(current_chunk)
            current_chunk = []
            current_size = 0

        current_chunk.append(category)
        current_size += cat_size

    if current_chunk:
        chunks.append(current_chunk)

    logger.info(f"DÃ©coupage en {len(chunks)} chunks basÃ© sur la taille estimÃ©e")

    for i, chunk in enumerate(chunks):
        logger.info(f"Traitement du chunk {i + 1}/{len(chunks)} ({len(chunk)} catÃ©gories)")
        try:
            chunk_data = {
                "categories": chunk,
                "brands": brands,
                "global_filters": global_filters
            }
            chunk_prompt = build_catalog_prompt(chunk_data)
            optimized_chunk = optimize_catalog_via_api(chunk_prompt)

            merged_data['categories'].extend(optimized_chunk.get('categories', []))

            if i == 0:
                merged_data['brands'] = optimized_chunk.get('brands', [])
                merged_data['filters'] = optimized_chunk.get('filters', {})

        except Exception as e:
            logger.error(f"Ã‰chec du chunk {i + 1}: {str(e)}")
            logger.info("Poursuite du traitement avec les chunks restants")

    seen_categories = set()
    deduped_categories = []
    for cat in merged_data['categories']:
        cat_id = cat.get('id')
        cat_name = cat.get('name', '')
        identifier = f"{cat_id}-{cat_name}" if cat_id else cat_name

        if identifier and identifier not in seen_categories:
            seen_categories.add(identifier)
            deduped_categories.append(cat)
    merged_data['categories'] = deduped_categories

    # CORRECTION CRITIQUE: DÃ©duplication correcte des marques
    brands_dict = {}
    for brand in merged_data['brands']:
        name = brand.get('name', '').lower()
        if name and name not in brands_dict:
            brands_dict[name] = brand

    merged_data['brands'] = sorted(
        brands_dict.values(),
        key=lambda x: x.get('name', '').lower()
    )

    logger.info(f"Fusion rÃ©ussie: {len(merged_data['categories'])} catÃ©gories, {len(merged_data['brands'])} marques")
    return merged_data


# ===== FONCTIONS EXISTANTES CONSERVÃ‰ES =====
def get_retry_delay(error, attempt):
    """Retourne le dÃ©lai de rÃ©essai intelligent selon le type d'erreur"""
    jitter = random.uniform(-5, 5)

    if isinstance(error, HTTPError):
        status_code = error.response.status_code
        if status_code == 429:
            return 90 + jitter
        elif status_code == 400:
            return None
        elif 500 <= status_code < 600:
            return 30 + jitter
    elif isinstance(error, Timeout):
        return 60 + jitter

    return (2 ** attempt) * 10 + jitter


def calculate_timeout(prompt_length, model):
    """Calcule dynamiquement le timeout"""
    base_timeout = 60
    size_factor = max(1, prompt_length / 1000)
    model_factor = 1.5 if "reasoner" in model else 1.0
    return min(300, int(base_timeout * size_factor * model_factor))


def validate_json_structure(data):
    """Valide la structure ET les types de donnÃ©es du JSON optimisÃ©"""
    required_structure = {
        "timestamp": int,
        "categories": list,
        "brands": list,
        "filters": dict,
        "global_filters": dict
    }

    errors = []

    # VÃ©rification des clÃ©s prÃ©sentes
    missing_keys = [key for key in required_structure if key not in data]
    if missing_keys:
        errors.append(f"ClÃ©s manquantes: {', '.join(missing_keys)}")

    # VÃ©rification des types de donnÃ©es
    for key, expected_type in required_structure.items():
        if key in data and not isinstance(data[key], expected_type):
            actual_type = type(data[key]).__name__
            errors.append(f"Type incorrect pour '{key}': attendu {expected_type.__name__}, obtenu {actual_type}")

    # VÃ©rification spÃ©cifique des catÃ©gories
    if "categories" in data:
        for i, category in enumerate(data["categories"]):
            if not isinstance(category, dict):
                errors.append(f"CatÃ©gorie #{i + 1} n'est pas un objet")
            elif "name" not in category or not category["name"]:
                errors.append(f"CatÃ©gorie #{i + 1} manque un nom valide")

    if errors:
        raise ValueError("\n".join(errors))

    return True


def extract_json_from_text(text):
    """Tente d'extraire un JSON valide de diffÃ©rents formats"""
    # Essai 1: JSON brut
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # Essai 2: JSON encapsulÃ© dans markdown
    md_match = re.search(r'```json\n(.*?)\n```', text, re.DOTALL)
    if md_match:
        try:
            return json.loads(md_match.group(1))
        except json.JSONDecodeError:
            pass

    # Essai 3: JSON encapsulÃ© dans code
    code_match = re.search(r'```(.*?)```', text, re.DOTALL)
    if code_match:
        try:
            return json.loads(code_match.group(1))
        except json.JSONDecodeError:
            pass

    # Essai 4: Tout objet JSON dans le texte
    json_match = re.search(r'\{[\s\S]*\}', text)
    if json_match:
        try:
            return json.loads(json_match.group(0))
        except json.JSONDecodeError:
            pass

    # Essai 5: Format YAML (conversion simple)
    if ":" in text and "- " in text:
        try:
            fixed_text = text.replace("'", '"')
            fixed_text = re.sub(r'(\w+):', r'"\1":', fixed_text)
            fixed_text = re.sub(r':\s+([^"\s]+)', r': "\1"', fixed_text)
            return json.loads(f"{{{fixed_text}}}")
        except Exception:
            pass

    raise ValueError("Aucun format JSON valide dÃ©tectÃ© dans la rÃ©ponse")


def optimize_catalog():
    """Orchestrateur principal pour l'optimisation du catalogue"""
    logger.info("DÃ©marrage de l'optimisation du catalogue")
    logger.info(f"ModÃ¨le sÃ©lectionnÃ©: {DEEPSEEK_MODEL}")

    try:
        with open(TEMP_FILE, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
            logger.info(f"DonnÃ©es brutes chargÃ©es : {len(raw_data.get('categories', []))} catÃ©gories")

        os.makedirs(CACHE_DIR, exist_ok=True)

        json_str = json.dumps(raw_data, sort_keys=True, ensure_ascii=False)
        current_hash = hashlib.sha256(json_str.encode('utf-8')).hexdigest()

        if os.path.exists(CACHE_FILE):
            try:
                with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)

                if cache_data.get('hash') == current_hash:
                    optimized_data = cache_data.get('data')
                    if optimized_data:
                        logger.info("Cache valide trouvÃ©. Utilisation des donnÃ©es optimisÃ©es en cache.")
                        return save_final_data(optimized_data)
            except Exception as e:
                logger.warning(f"Erreur lors de la lecture du cache : {str(e)}")

    except FileNotFoundError:
        logger.error(f"Fichier {TEMP_FILE} introuvable")
        return False
    except json.JSONDecodeError:
        logger.error("Erreur de dÃ©codage JSON - fichier corrompu")
        return False
    except Exception as e:
        logger.error(f"Erreur lors du chargement du fichier : {str(e)}")
        return False

    optimized_data = None
    for attempt in range(MAX_RETRIES):
        try:
            optimized_data = process_catalog_chunks(raw_data)
            if optimized_data:
                break
        except Exception as e:
            logger.warning(f"Tentative {attempt + 1} Ã©chouÃ©e: {str(e)}")
            delay = get_retry_delay(e, attempt)
            if delay is None:
                logger.error("ArrÃªt des tentatives (erreur client irrÃ©cupÃ©rable)")
                break

            if attempt < MAX_RETRIES - 1:
                logger.info(f"RÃ©essai dans {delay:.1f} secondes...")
                time.sleep(delay)

    if not optimized_data:
        logger.error("Ã‰chec de l'optimisation aprÃ¨s %d tentatives", MAX_RETRIES)
        return False

    try:
        cache_data = {
            "hash": current_hash,
            "data": optimized_data,
            "timestamp": time.time()
        }
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False)
        logger.info("Cache mis Ã  jour avec succÃ¨s")
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde du cache : {str(e)}")

    return save_final_data(optimized_data)


def save_final_data(optimized_data):
    """Sauvegarde et validation des donnÃ©es optimisÃ©es"""
    try:
        validate_json_structure(optimized_data)

        with open(FINAL_FILE, 'w', encoding='utf-8') as f:
            json.dump(optimized_data, f, indent=2, ensure_ascii=False)
            logger.info(f"Catalogue optimisÃ© sauvegardÃ© dans {FINAL_FILE}")
        return True
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde : {str(e)}")
        return False


def build_catalog_prompt(raw_data):
    """Construit un prompt texte clair Ã  partir des donnÃ©es brutes du catalogue"""
    return PROMPT_OPTIMISATION_CATALOGUE


# Prompt pour descriptions produits
PROMPT_COMMUN = (
    "Tu es un expert en mode et ventes de vÃªtements sur Vinted.\n"
    "RÃ©dige une description courte, naturelle, fluide et professionnelle.\n"
    "Indique : marque, couleur, coupe, style gÃ©nÃ©ral (casual, sport, vintage, streetwear).\n"
    "SuggÃ¨re une idÃ©e simple pour le porter (ex : jean, baskets).\n"
    "PrÃ©cise : trÃ¨s bon Ã©tat, prÃªt Ã  porter.\n"
    "Si tu connais la taille, indique-la sous la forme 'Taille : M'. Sinon, n'indique rien.\n"
    "Termine toujours par deux phrases distinctes sur deux lignes :\n"
    "'EnvoyÃ© sous 24h, lavÃ© et repassÃ©.'\n"
    "'N'hÃ©site pas si tu as des questions.'\n"
    "Pas d'emojis, pas de hashtags, pas de mise en forme spÃ©ciale.\n"
    "Fais des retours Ã  la ligne pour une meilleure lisibilitÃ©."
)

if __name__ == "__main__":
    optimize_catalog()